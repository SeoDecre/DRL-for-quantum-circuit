{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637e0cc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'github_auth_patch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# =================== CRITICAL: THIS MUST BE FIRST ===================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Import github_auth_patch BEFORE anything else to enable authentication\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgithub_auth_patch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# ======================================================================\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'github_auth_patch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque, Counter\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "from qsimbench import get_outcomes, get_index\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from ast import literal_eval\n",
    "\n",
    "# --- Oracle and caching system ---\n",
    "# This section defines the \"teacher\" for our RL agent. \n",
    "# The Oracle determines the correct answer (the optimal number of shots), which is used to score the agent's performance and provide a learning signal (the reward)\n",
    "\n",
    "ORACLE_CACHE = {}\n",
    "CACHE_FILE = \"oracle_cache_enhanced.json\"\n",
    "\n",
    "def save_oracle_cache():\n",
    "    \"\"\"\n",
    "    Saves the dictionary of computed optimal shots to a JSON file\n",
    "    This is a critical optimization, as running the Oracle is the most time-consuming part of the program\n",
    "    Saving the results allows us to run the Oracle only once for the entire dataset\n",
    "    \"\"\"\n",
    "    # JSON cannot serialize tuple keys, so we convert them to strings first\n",
    "    string_key_cache = {str(k): v for k, v in ORACLE_CACHE.items()}\n",
    "    with open(CACHE_FILE, 'w') as f:\n",
    "        json.dump(string_key_cache, f)\n",
    "    print(f\"Oracle cache with {len(ORACLE_CACHE)} items saved to {CACHE_FILE}\")\n",
    "\n",
    "def load_oracle_cache():\n",
    "    \"\"\"\n",
    "    Loads the pre-computed optimal shots from the cache file if it exists\n",
    "    This dramatically speeds up subsequent training runs\n",
    "    \"\"\"\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        try:\n",
    "            with open(CACHE_FILE, 'r') as f:\n",
    "                string_key_cache = json.load(f)\n",
    "                # Convert the string keys from the JSON file back into their original tuple format\n",
    "                for k_str, v in string_key_cache.items():\n",
    "                    ORACLE_CACHE[literal_eval(k_str)] = v\n",
    "            print(f\"Loaded {len(ORACLE_CACHE)} items from oracle cache.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not read cache file '{CACHE_FILE}'. Starting empty. Error: {e}\")\n",
    "\n",
    "def find_optimal_shots(algorithm: str, size: int, backend: str, step_size=50, max_shots=20000, threshold=0.01, stability_k=3):\n",
    "    \"\"\"\n",
    "    ================================================================================\n",
    "    *** THE ORACLE ***\n",
    "    ================================================================================\n",
    "    PURPOSE:\n",
    "        This function serves as the \"teacher\" by implementing the Incremental Execution (IE) algorithm described in the paper\n",
    "        It iteratively runs batches of shots and uses a statistical metric to find the point of convergence, which we define as the \"optimal\" shot count.\n",
    "\n",
    "    ROLE IN THE SYSTEM:\n",
    "        Its output (`optimal_shots`) is used ONLY to calculate the final reward for the RL agent during training and as a benchmark during evaluation\n",
    "        The agent itself never gets to see this value while it is making decisions\n",
    "\n",
    "    METHOD:\n",
    "        It determines convergence by measuring the Total Variation Distance (TVD) between the cumulative probability distribution of outcomes at consecutive steps\n",
    "        If the TVD remains below a `threshold` for `stability_k` consecutive iterations, the distribution is considered stable, and the process stops\n",
    "    \"\"\"\n",
    "    # Check if we have already computed this value to save time\n",
    "    cache_key = (algorithm, size, backend)\n",
    "    if cache_key in ORACLE_CACHE:\n",
    "        return ORACLE_CACHE[cache_key]\n",
    "\n",
    "    cumulative_counts = Counter()  # Stores the aggregated outcomes (e.g., {'01': 10, '10': 12})\n",
    "    stable_iterations = 0          # Counter for consecutive stable steps\n",
    "    prev_dist = {}                 # Stores the probability distribution from the previous step\n",
    "\n",
    "    def normalize_dist(counts: Dict[str, int]) -> Dict[str, float]:\n",
    "        \"\"\"Converts raw counts into a normalized probability distribution\"\"\"\n",
    "        total = sum(counts.values())\n",
    "        return {k: v / total for k, v in counts.items()} if total > 0 else {}\n",
    "\n",
    "    def total_variation_distance(p: Dict[str, float], q: Dict[str, float]) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the TVD between two probability distributions, p and q\n",
    "        TVD is a metric of the distance between two distributions\n",
    "        A small TVD (close to 0) means the distributions are very similar\n",
    "        The formula is: TVD(p, q) = 0.5 * sum(|p(i) - q(i)| for all outcomes i)\n",
    "        \"\"\"\n",
    "        all_keys = set(p) | set(q) # Consider all outcomes present in either distribution\n",
    "        return 0.5 * sum(abs(p.get(k, 0) - q.get(k, 0)) for k in all_keys)\n",
    "\n",
    "    # Main loop of the Incremental Execution algorithm\n",
    "    for total_shots_so_far in range(step_size, max_shots + step_size, step_size):\n",
    "        try:\n",
    "            # Use qsimbench to simulate running a new batch of shots\n",
    "            new_batch_counts = get_outcomes(algorithm, size, backend, shots=step_size, strategy='random', exact=True)\n",
    "        except Exception:\n",
    "            # In case of an error, fall back to a maximum value\n",
    "            ORACLE_CACHE[cache_key] = max_shots\n",
    "            return max_shots\n",
    "\n",
    "        # Accumulate the new outcomes\n",
    "        cumulative_counts.update(new_batch_counts)\n",
    "        current_dist = normalize_dist(cumulative_counts)\n",
    "\n",
    "        # Compute convergence metrics\n",
    "        if prev_dist:\n",
    "            tvd = total_variation_distance(current_dist, prev_dist)\n",
    "            if tvd < threshold:\n",
    "                stable_iterations += 1\n",
    "                if stable_iterations >= stability_k:\n",
    "                    # Convergence achieved!\n",
    "                    ORACLE_CACHE[cache_key] = total_shots_so_far\n",
    "                    return total_shots_so_far\n",
    "            else:\n",
    "                # Distribution changed significantly, reset stability counter\n",
    "                stable_iterations = 0\n",
    "\n",
    "        prev_dist = current_dist\n",
    "\n",
    "    # If we reach this point, convergence was not achieved, so return the maximum\n",
    "    ORACLE_CACHE[cache_key] = max_shots\n",
    "    return max_shots\n",
    "\n",
    "# --- Quantum environment ---\n",
    "\n",
    "class IterativeQuantumEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    ENHANCEMENTS IN THIS VERSION:\n",
    "    1. Difficulty-biased sampling: The environment oversamples harder problems (those requiring more shots)\n",
    "    2. Enhanced state features: Added entropy and variance metrics to help the agent distinguish between similar-looking problems\n",
    "    \"\"\"\n",
    "    def __init__(self, max_shots=20000, step_size=50, hard_problem_bias=0.7):\n",
    "        super().__init__()\n",
    "        self.max_shots = max_shots\n",
    "        self.step_size = step_size\n",
    "        self.hard_problem_bias = hard_problem_bias  # Probability of sampling a \"hard\" problem (70%)\n",
    "        self.index = get_index()  # Get all available problems from qsimbench\n",
    "        self.all_triplets = self._get_all_noisy_triplets()\n",
    "        self.alg_map, self.backend_map = self._create_mappings()\n",
    "\n",
    "        print(\"Pre-computing optimal shots for all triplets using the oracle...\")\n",
    "        # This loop populates the oracle cache to ensure training is fast.\n",
    "        for triplet in tqdm(self.all_triplets, desc=\"Oracle Pre-computation\"):\n",
    "            find_optimal_shots(triplet[0], triplet[1], triplet[2])\n",
    "        print(\"Oracle pre-computation complete.\")\n",
    "        \n",
    "        # Categorize problems by difficulty\n",
    "        self._categorize_by_difficulty()\n",
    "\n",
    "        # Action space: The agent can choose between two discrete actions\n",
    "        # 0: CONTINUE (run another batch of shots)\n",
    "        # 1: STOP (end the episode)\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        # Observation space is now 8-dimensional instead of 4\n",
    "        # We've added features to help the agent better distinguish between problems\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "        \n",
    "        # Metrics tracking for the current episode\n",
    "        self.outcome_history = []  # Stores distributions from each step for additional metrics\n",
    "\n",
    "    def _get_all_noisy_triplets(self) -> List[Tuple[str, int, str]]:\n",
    "        \"\"\"Helper function to parse the qsimbench index and find all valid noisy problems\"\"\"\n",
    "        triplets = []\n",
    "        for alg, sizes in self.index.items():\n",
    "            for size, backends in sizes.items():\n",
    "                if \"aer_simulator\" in backends: # Ensure an ideal, noiseless simulator exists for comparison\n",
    "                    for b in backends:\n",
    "                        if b != \"aer_simulator\": # We only want to train on noisy, realistic backends\n",
    "                            triplets.append((alg, int(size), b))\n",
    "        return triplets\n",
    "\n",
    "    def _create_mappings(self) -> Tuple[Dict[str, int], Dict[str, int]]:\n",
    "        \"\"\"Creates dictionaries to map string names (e.g., 'qaoa') to integer indices for normalization\"\"\"\n",
    "        all_algs = sorted(list(set(t[0] for t in self.all_triplets)))\n",
    "        all_backends = sorted(list(set(t[2] for t in self.all_triplets)))\n",
    "        alg_map = {name: i for i, name in enumerate(all_algs)}\n",
    "        backend_map = {name: i for i, name in enumerate(all_backends)}\n",
    "        return alg_map, backend_map\n",
    "\n",
    "    def _categorize_by_difficulty(self, difficulty_threshold=5000):\n",
    "        \"\"\"\n",
    "        ENHANCEMENT #1 - Difficulty-Based Sampling\n",
    "        \n",
    "        Categorizes all problems into \"easy\" and \"hard\" based on their optimal shot count\n",
    "        This allows us to bias our sampling toward harder problems during training\n",
    "        \n",
    "        WHY THIS HELPS:\n",
    "        - The agent is currently struggling with problems that require many shots (>5000)\n",
    "        - By increasing exposure to these difficult cases, the agent gets more practice on its weaknesses\n",
    "        - This is a form of \"curriculum learning\" or \"hard example mining\"\n",
    "        \"\"\"\n",
    "        self.easy_triplets = []\n",
    "        self.hard_triplets = []\n",
    "        \n",
    "        for triplet in self.all_triplets:\n",
    "            optimal = find_optimal_shots(*triplet)\n",
    "            if optimal > difficulty_threshold:\n",
    "                self.hard_triplets.append(triplet)\n",
    "            else:\n",
    "                self.easy_triplets.append(triplet)\n",
    "        \n",
    "        print(f\"Categorized {len(self.hard_triplets)} hard problems (>{difficulty_threshold} shots)\")\n",
    "        print(f\"Categorized {len(self.easy_triplets)} easy problems (<={difficulty_threshold} shots)\")\n",
    "        print(f\"Hard problem bias set to {self.hard_problem_bias:.1%}\")\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Now uses biased sampling to select harder problems more frequently\n",
    "        \"\"\"\n",
    "        # Biased sampling logic\n",
    "        if random.random() < self.hard_problem_bias and len(self.hard_triplets) > 0:\n",
    "            # Sample from hard problems\n",
    "            self.current_triplet = random.choice(self.hard_triplets)\n",
    "        else:\n",
    "            # Sample from all problems (maintaining some easy ones to prevent overfitting to hard cases)\n",
    "            self.current_triplet = random.choice(self.all_triplets)\n",
    "        \n",
    "        # Get the pre-computed optimal shots for this problem from the Oracle's cache\n",
    "        self.optimal_shots = find_optimal_shots(*self.current_triplet)\n",
    "        self.current_shots = 0\n",
    "        \n",
    "        # Reset outcome history for computing advanced metrics\n",
    "        self.outcome_history = []\n",
    "        \n",
    "        return self._get_state() # Return the initial state of the new episode\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:\n",
    "        \"\"\"\n",
    "        Processes one step of the game based on the agent's chosen action\n",
    "        \"\"\"\n",
    "        done = False\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        if action == 1:  # Agent chose to STOP\n",
    "            # Terminate the episode and calculate the final, large reward\n",
    "            state, reward, done, info = self._terminate()\n",
    "        else:  # Agent chose to CONTINUE\n",
    "            self.current_shots += self.step_size\n",
    "            \n",
    "            # Collect outcome data for enhanced state features\n",
    "            try:\n",
    "                alg, size, backend = self.current_triplet\n",
    "                batch_outcomes = get_outcomes(alg, size, backend, shots=self.step_size, strategy='random', exact=True)\n",
    "                self.outcome_history.append(batch_outcomes)\n",
    "            except:\n",
    "                pass  # If sampling fails, continue without updating history\n",
    "            \n",
    "            if self.current_shots >= self.max_shots:\n",
    "                # If we hit the shot limit, the episode ends automatically\n",
    "                state, reward, done, info = self._terminate()\n",
    "            else:\n",
    "                # If we continue, give a small negative reward (penalty) to encourage\n",
    "                # the agent to finish episodes faster and be more efficient\n",
    "                reward = -0.02\n",
    "                state = self._get_state()\n",
    "\n",
    "        return state, reward, done, info\n",
    "\n",
    "    def _terminate(self) -> Tuple[np.ndarray, float, bool, Dict]:\n",
    "        \"\"\"\n",
    "        Ends the episode and calculates the final reward based on performance\n",
    "        This is where the agent receives its main learning signal\n",
    "        \"\"\"\n",
    "        # Calculate the error: positive for overshooting, negative for undershooting\n",
    "        error = self.current_shots - self.optimal_shots\n",
    "\n",
    "        # --- Asymmetric Reward Shaping Logic ---\n",
    "        # The design of the reward is crucial for guiding the agent's behavior\n",
    "        if error < 0:\n",
    "            # SEVERE PENALTY FOR UNDERSHOOTING: Stopping too early is a critical failure because the result is not statistically stable\n",
    "            # The penalty is proportional to how badly it undershot, teaching the agent that this is the worst possible outcome\n",
    "            final_reward = -1.0 - (abs(error) / self.optimal_shots)\n",
    "        else:\n",
    "            # DECAYING REWARD FOR OVERSHOOTING: This is considered a success, but it becomes less successful as more shots are wasted\n",
    "            # The reward is +1.0 for a perfect stop (error=0) and decays exponentially, encouraging the agent to be precise\n",
    "            final_reward = np.exp(-0.0005 * error)\n",
    "\n",
    "        state = self._get_state()\n",
    "        done = True\n",
    "        info = {\n",
    "            'shots_used': self.current_shots,\n",
    "            'optimal_shots': self.optimal_shots,\n",
    "            'error': error,\n",
    "            'triplet': self.current_triplet,\n",
    "            'final_reward': final_reward\n",
    "        }\n",
    "        return state, final_reward, done, info\n",
    "\n",
    "    def _compute_distribution_entropy(self, outcomes: Dict[str, int]) -> float:\n",
    "        \"\"\"\n",
    "        ================================================================================\n",
    "        SHANNON ENTROPY COMPUTATION (Feature 5)\n",
    "        ================================================================================\n",
    "        \n",
    "        Computes the Shannon entropy of a probability distribution derived from\n",
    "        measurement outcomes.\n",
    "        \n",
    "        MATHEMATICAL FORMULA:\n",
    "            H(X) = -Σ p(x) * log₂(p(x))  for all outcomes x\n",
    "        \n",
    "        WHERE:\n",
    "            - p(x) = count(x) / total_counts  (probability of outcome x)\n",
    "            - The sum is taken over all unique measurement outcomes\n",
    "        \n",
    "        COMPUTATION STEPS:\n",
    "            1. Normalize raw counts into probabilities\n",
    "               Input:  {'00': 55, '01': 22, '10': 13, '11': 10}  (total=100)\n",
    "               Output: {'00': 0.55, '01': 0.22, '10': 0.13, '11': 0.10}\n",
    "            \n",
    "            2. Apply Shannon entropy formula for each outcome:\n",
    "               For '00': -0.55 * log₂(0.55) ≈ 0.528\n",
    "               For '01': -0.22 * log₂(0.22) ≈ 0.474\n",
    "               For '10': -0.13 * log₂(0.13) ≈ 0.382\n",
    "               For '11': -0.10 * log₂(0.10) ≈ 0.332\n",
    "               Total: H ≈ 1.716 bits\n",
    "            \n",
    "            3. Normalize by maximum possible entropy:\n",
    "               - For n qubits, max entropy = log₂(2^n) = n bits\n",
    "               - We use a cap of 4 bits (assumes ≤4 qubit circuits)\n",
    "               - Normalized value: 1.716 / 4.0 ≈ 0.429\n",
    "        \n",
    "        INTERPRETATION:\n",
    "            - RETURN VALUE in [0, 1]\n",
    "            - 0.0 = Deterministic (one outcome has probability 1)\n",
    "            - 1.0 = Maximum entropy (perfectly uniform distribution)\n",
    "            - 0.5 = Moderate spread\n",
    "        \n",
    "        WHY THIS HELPS THE AGENT:\n",
    "            - Entropy captures the \"shape\" of the distribution\n",
    "            - High entropy -> uniform superposition -> might need more shots for precision\n",
    "            - Low entropy -> peaked distribution -> might converge faster\n",
    "            - Different algorithms produce different entropy signatures:\n",
    "              * Grover's algorithm: low entropy (searching for specific state)\n",
    "              * Random circuit sampling: high entropy (uniform superposition)\n",
    "        \"\"\"\n",
    "        total = sum(outcomes.values())\n",
    "        if total == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        entropy = 0.0\n",
    "        for count in outcomes.values():\n",
    "            if count > 0:\n",
    "                p = count / total  # Compute probability p(x)\n",
    "                entropy -= p * np.log2(p)  # Accumulate: -p*log₂(p)\n",
    "        \n",
    "        # Normalize by theoretical maximum entropy for a 2-qubit system (adjust if needed)\n",
    "        # For n qubits, max entropy is log2(2^n) = n\n",
    "        # We'll assume a reasonable upper bound of 4 qubits = 4 bits of entropy\n",
    "        max_entropy = 4.0\n",
    "        return min(entropy / max_entropy, 1.0)  # Clip to [0, 1]\n",
    "\n",
    "    def _compute_distribution_variance(self, outcomes: Dict[str, int]) -> float:\n",
    "        \"\"\"\n",
    "        ================================================================================\n",
    "        DISTRIBUTION VARIANCE COMPUTATION (Feature 6)\n",
    "        ================================================================================\n",
    "        \n",
    "        Computes the variance of the probability distribution as a measure of spread.\n",
    "        \n",
    "        MATHEMATICAL FORMULA:\n",
    "            Var(P) = (1/n) * Σ(pᵢ - μ)²\n",
    "        \n",
    "        WHERE:\n",
    "            - pᵢ = probability of outcome i\n",
    "            - μ = mean probability = (1/n) for uniform distribution\n",
    "            - n = number of unique outcomes\n",
    "        \n",
    "        COMPUTATION STEPS:\n",
    "            1. Convert counts to probabilities:\n",
    "               Input:  {'00': 90, '01': 5, '10': 3, '11': 2}  (total=100)\n",
    "               Output: [0.90, 0.05, 0.03, 0.02]\n",
    "            \n",
    "            2. Calculate mean probability:\n",
    "               mean_p = (0.90 + 0.05 + 0.03 + 0.02) / 4 = 0.25\n",
    "            \n",
    "            3. Compute variance:\n",
    "               Var = [(0.90-0.25)² + (0.05-0.25)² + (0.03-0.25)² + (0.02-0.25)²] / 4\n",
    "                   = [0.4225 + 0.04 + 0.0484 + 0.0529] / 4\n",
    "                   ≈ 0.141\n",
    "            \n",
    "            4. Scale and normalize to [0, 1]:\n",
    "               scaled_variance = min(0.141 * 10.0, 1.0) ≈ 1.0\n",
    "        \n",
    "        INTERPRETATION:\n",
    "            - RETURN VALUE in [0, 1]\n",
    "            - 0.0 = Uniform distribution (all probabilities equal)\n",
    "            - 1.0 = Highly peaked (one dominant outcome)\n",
    "            - Intermediate values = Partially peaked distribution\n",
    "        \n",
    "        WHY THIS COMPLEMENTS ENTROPY:\n",
    "            - Entropy measures \"uniformity\" globally\n",
    "            - Variance measures \"peakedness\" (distance from mean)\n",
    "            - Example where they differ:\n",
    "              * {'00': 0.7, '01': 0.1, '10': 0.1, '11': 0.1}\n",
    "              * Entropy: moderate (some spread)\n",
    "              * Variance: high (one outcome dominates)\n",
    "            - Together, they give a richer picture of distribution shape\n",
    "        \n",
    "        NORMALIZATION NOTE:\n",
    "            - The scaling factor of 10.0 is empirical\n",
    "            - Maximum theoretical variance for k outcomes ≈ 1/k\n",
    "            - This scaling brings typical variances into the [0, 1] range\n",
    "        \"\"\"\n",
    "        if not outcomes:\n",
    "            return 0.0\n",
    "        \n",
    "        total = sum(outcomes.values())\n",
    "        if total == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Step 1: Convert to probabilities\n",
    "        probs = [count / total for count in outcomes.values()]\n",
    "        \n",
    "        # Step 2 & 3: Calculate mean and variance\n",
    "        mean_prob = np.mean(probs)\n",
    "        variance = np.var(probs)  # NumPy computes: (1/n)Σ(pᵢ - μ)²\n",
    "        \n",
    "        # Step 4: Normalize\n",
    "        # Max variance occurs when one probability is 1, others are 0\n",
    "        # For a distribution with k outcomes, max variance ≈ 1/k (when perfectly unbalanced)\n",
    "        # We use a conservative scaling factor to map typical values to [0, 1]\n",
    "        return min(variance * 10.0, 1.0)\n",
    "\n",
    "    def _compute_rate_of_change(self) -> float:\n",
    "        \"\"\"\n",
    "        ================================================================================\n",
    "        RATE OF CHANGE COMPUTATION (Feature 7)\n",
    "        ================================================================================\n",
    "        \n",
    "        Estimates how rapidly the probability distribution is evolving by measuring\n",
    "        the Total Variation Distance (TVD) between recent and previous distributions.\n",
    "        \n",
    "        MATHEMATICAL FORMULA:\n",
    "            TVD(P, Q) = 0.5 * Σ|P(x) - Q(x)|  for all outcomes x\n",
    "        \n",
    "        WHERE:\n",
    "            - P = probability distribution from all previous batches (cumulative)\n",
    "            - Q = probability distribution from the most recent batch\n",
    "            - The sum is over all possible measurement outcomes\n",
    "        \n",
    "        COMPUTATION STEPS:\n",
    "            1. Separate outcome history into two parts:\n",
    "               - Previous: All batches except the last\n",
    "                 Example: [{'00': 25, '01': 15, '10': 7, '11': 3},\n",
    "                          {'00': 28, '01': 12, '10': 6, '11': 4}]\n",
    "                 Aggregate: {'00': 53, '01': 27, '10': 13, '11': 7} (total=100)\n",
    "               \n",
    "               - Recent: Only the last batch\n",
    "                 Example: {'00': 30, '01': 10, '10': 5, '11': 5} (total=50)\n",
    "            \n",
    "            2. Normalize both to probability distributions:\n",
    "               - prev_dist = {'00': 0.53, '01': 0.27, '10': 0.13, '11': 0.07}\n",
    "               - recent_dist = {'00': 0.60, '01': 0.20, '10': 0.10, '11': 0.10}\n",
    "            \n",
    "            3. Calculate TVD:\n",
    "               TVD = 0.5 * (|0.53-0.60| + |0.27-0.20| + |0.13-0.10| + |0.07-0.10|)\n",
    "                   = 0.5 * (0.07 + 0.07 + 0.03 + 0.03)\n",
    "                   = 0.5 * 0.20\n",
    "                   = 0.10\n",
    "        \n",
    "        INTERPRETATION:\n",
    "            - RETURN VALUE in [0, 1]\n",
    "            - 1.0 = Maximum change (distributions are completely different)\n",
    "            - 0.0 = No change (distributions are identical -> convergence!)\n",
    "            - 0.10 = Small change (10% difference, approaching stability)\n",
    "        \n",
    "        WHY THIS IS CRITICAL FOR THE AGENT:\n",
    "            - This is a \"convergence velocity\" metric\n",
    "            - EARLY IN EXECUTION:\n",
    "              * Distribution changes significantly with each new batch\n",
    "              * TVD might be 0.15-0.30 (15-30% change)\n",
    "              * Signal to agent: \"Keep going, not stable yet\"\n",
    "            \n",
    "            - NEAR CONVERGENCE:\n",
    "              * Distribution barely changes with new batches\n",
    "              * TVD drops below 0.01 (1% change)\n",
    "              * Signal to agent: \"Safe to stop, distribution is stable\"\n",
    "            \n",
    "            - This is exactly what the Incremental Execution algorithm monitors\n",
    "            - By learning this feature, the agent can anticipate convergence\n",
    "        \n",
    "        SPECIAL CASES:\n",
    "            - If len(outcome_history) < 2: Return 1.0 (maximum uncertainty)\n",
    "            - If total counts are 0: Return 1.0 (no data to compare)\n",
    "        \n",
    "        RELATIONSHIP TO ORACLE:\n",
    "            - The Oracle stops when TVD < threshold for k consecutive iterations\n",
    "            - This feature gives the agent direct access to the TVD signal\n",
    "            - But the agent must learn the appropriate threshold through RL\n",
    "        \"\"\"\n",
    "        if len(self.outcome_history) < 2:\n",
    "            # Not enough data to compute rate of change\n",
    "            return 1.0  # Maximum rate of change (unknown state)\n",
    "        \n",
    "        # Step 1: Aggregate all previous batches (excluding the most recent)\n",
    "        prev_outcomes = Counter()\n",
    "        for outcomes in self.outcome_history[:-1]:\n",
    "            prev_outcomes.update(outcomes)\n",
    "        \n",
    "        # Get the most recent batch\n",
    "        recent_outcomes = self.outcome_history[-1]\n",
    "        \n",
    "        # Step 2: Normalize both to probability distributions\n",
    "        prev_total = sum(prev_outcomes.values())\n",
    "        recent_total = sum(recent_outcomes.values())\n",
    "        \n",
    "        if prev_total == 0 or recent_total == 0:\n",
    "            return 1.0  # Cannot compute meaningful distance\n",
    "        \n",
    "        prev_dist = {k: v/prev_total for k, v in prev_outcomes.items()}\n",
    "        recent_dist = {k: v/recent_total for k, v in recent_outcomes.items()}\n",
    "        \n",
    "        # Step 3: Compute Total Variation Distance (TVD)\n",
    "        # Must consider all outcomes that appear in either distribution\n",
    "        all_keys = set(prev_dist.keys()) | set(recent_dist.keys())\n",
    "        tvd = 0.5 * sum(abs(prev_dist.get(k, 0) - recent_dist.get(k, 0)) for k in all_keys)\n",
    "        \n",
    "        return tvd  # Already in [0, 1] range\n",
    "\n",
    "    def _get_state(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        ================================================================================\n",
    "        FEATURE VECTOR CONSTRUCTION - 8-Dimensional State Representation\n",
    "        ================================================================================\n",
    "        \n",
    "        This function constructs the state observation that the RL agent uses to make\n",
    "        decisions. The state is an 8-dimensional vector where each feature is normalized\n",
    "        to the range [0, 1] to facilitate neural network training.\n",
    "        \n",
    "        ORIGINAL 4 FEATURES:\n",
    "        1. Algorithm (normalized index)\n",
    "        2. Problem size (normalized)\n",
    "        3. Backend (normalized index)\n",
    "        4. Current shots (normalized)\n",
    "        \n",
    "        NEW 4 ADDITIONAL FEATURES:\n",
    "        5. Distribution entropy (measures spread of outcomes)\n",
    "        6. Distribution variance (alternative spread metric)\n",
    "        7. Rate of change (how fast distribution is evolving)\n",
    "        8. Relative progress (current_shots / typical_convergence_point)\n",
    "        \"\"\"\n",
    "        alg, size, backend = self.current_triplet\n",
    "\n",
    "        # ============================================================================\n",
    "        # FEATURE 1: ALGORITHM TYPE (Normalized Categorical Encoding)\n",
    "        # ============================================================================\n",
    "        # HOW IT'S COMPUTED:\n",
    "        #   - Each algorithm (e.g., 'grover', 'qaoa', 'vqe') is assigned a unique integer\n",
    "        #     index during initialization via self.alg_map (e.g., {'grover': 0, 'qaoa': 1})\n",
    "        #   - This index is retrieved: self.alg_map.get(alg, 0)\n",
    "        #   - Normalized to [0, 1] by dividing by (total_algorithms - 1)\n",
    "        #     Example: If there are 3 algorithms, indices 0,1,2 become 0.0, 0.5, 1.0\n",
    "        # PURPOSE:\n",
    "        #   - Different quantum algorithms have fundamentally different convergence behaviors\n",
    "        #   - This feature allows the agent to learn algorithm-specific stopping strategies\n",
    "        # ============================================================================\n",
    "        alg_norm = self.alg_map.get(alg, 0) / (len(self.alg_map) - 1) if len(self.alg_map) > 1 else 0.5\n",
    "        \n",
    "        # ============================================================================\n",
    "        # FEATURE 2: PROBLEM SIZE (Normalized Continuous Value)\n",
    "        # ============================================================================\n",
    "        # HOW IT'S COMPUTED:\n",
    "        #   - Directly from the problem triplet: (algorithm, SIZE, backend)\n",
    "        #   - The size represents the number of qubits in the quantum circuit\n",
    "        #   - Normalized by dividing by an assumed maximum size of 15 qubits\n",
    "        #     (based on typical values in the qsimbench dataset)\n",
    "        # PURPOSE:\n",
    "        #   - Larger circuits generally require more shots to converge due to:\n",
    "        #     * Exponentially larger state space (2^n possible outcomes)\n",
    "        #     * More complex probability distributions\n",
    "        #   - This feature helps the agent adjust its patience based on problem scale\n",
    "        # NORMALIZATION EXAMPLE:\n",
    "        #   - 3-qubit circuit: 3/15 = 0.2\n",
    "        #   - 10-qubit circuit: 10/15 ≈ 0.67\n",
    "        # ============================================================================\n",
    "        size_norm = size / 15.0\n",
    "        \n",
    "        # ============================================================================\n",
    "        # FEATURE 3: BACKEND TYPE (Normalized Categorical Encoding)\n",
    "        # ============================================================================\n",
    "        # HOW IT'S COMPUTED:\n",
    "        #   - Each quantum backend (e.g., 'ibmq_lima', 'ionq_harmony') is assigned\n",
    "        #     a unique integer index via self.backend_map during initialization\n",
    "        #   - This index is retrieved: self.backend_map.get(backend, 0)\n",
    "        #   - Normalized to [0, 1] by dividing by (total_backends - 1)\n",
    "        # PURPOSE:\n",
    "        #   - Different hardware backends have distinct noise profiles:\n",
    "        #     * Superconducting qubits (IBM) vs. trapped ions (IonQ)\n",
    "        #     * Different gate fidelities, coherence times, and error rates\n",
    "        #   - These noise characteristics directly affect convergence speed\n",
    "        #   - This feature enables backend-specific learned policies\n",
    "        # ============================================================================\n",
    "        backend_norm = self.backend_map.get(backend, 0) / (len(self.backend_map) - 1) if len(self.backend_map) > 1 else 0.5\n",
    "        \n",
    "        # ============================================================================\n",
    "        # FEATURE 4: CURRENT SHOT COUNT (Normalized Progress Indicator)\n",
    "        # ============================================================================\n",
    "        # HOW IT'S COMPUTED:\n",
    "        #   - Directly from self.current_shots (accumulated across all batches)\n",
    "        #   - Normalized by dividing by self.max_shots (typically 20,000)\n",
    "        # PURPOSE:\n",
    "        #   - Critical feature for stopping decision: \"How many resources have I used?\"\n",
    "        #   - Provides a hard constraint awareness: must stop before reaching max_shots\n",
    "        #   - Helps agent learn when it's approaching the budget limit\n",
    "        # NORMALIZATION EXAMPLE:\n",
    "        #   - 1000 shots used: 1000/20000 = 0.05 (5% of budget)\n",
    "        #   - 15000 shots used: 15000/20000 = 0.75 (75% of budget, time to consider stopping)\n",
    "        # ============================================================================\n",
    "        shots_norm = self.current_shots / self.max_shots\n",
    "\n",
    "        # ============================================================================\n",
    "        # ENHANCED FEATURES (5-8): Statistical Properties of the Measured Distribution\n",
    "        # ============================================================================\n",
    "        # These features are computed from self.outcome_history, which stores the\n",
    "        # measurement results from each batch of shots executed so far\n",
    "        \n",
    "        if self.outcome_history:\n",
    "            # Aggregate all outcomes across all batches to get cumulative distribution\n",
    "            cumulative_outcomes = Counter()\n",
    "            for batch in self.outcome_history:\n",
    "                cumulative_outcomes.update(batch)\n",
    "            # Example result: {'00': 55, '01': 22, '10': 13, '11': 10}\n",
    "            \n",
    "            # ========================================================================\n",
    "            # FEATURE 5: SHANNON ENTROPY (Distribution Shape Metric)\n",
    "            # ========================================================================\n",
    "            # HOW IT'S COMPUTED:\n",
    "            #   1. Convert counts to probabilities: p_i = count_i / total_counts\n",
    "            #      Example: {'00': 0.55, '01': 0.22, '10': 0.13, '11': 0.10}\n",
    "            #   2. Apply Shannon entropy formula: H = -Σ(p_i * log₂(p_i))\n",
    "            #   3. Normalize by max entropy (log₂(2^n) = n qubits), capped at 4\n",
    "            # PURPOSE:\n",
    "            #   - Quantifies the \"uniformity\" of the probability distribution\n",
    "            #   - HIGH ENTROPY (->1.0): Outcomes are evenly distributed (uniform)\n",
    "            #     Example: {'00': 0.25, '01': 0.25, '10': 0.25, '11': 0.25} -> H≈2.0 (for 2 qubits)\n",
    "            #   - LOW ENTROPY (->0.0): Outcomes are concentrated in few states (peaked)\n",
    "            #     Example: {'00': 0.95, '01': 0.02, '10': 0.02, '11': 0.01} -> H≈0.3\n",
    "            #   - Helps distinguish between algorithms that produce uniform superpositions\n",
    "            #     (e.g., Hadamard-heavy circuits) vs. peaked distributions (e.g., optimized QAOA)\n",
    "            # ========================================================================\n",
    "            entropy = self._compute_distribution_entropy(cumulative_outcomes)\n",
    "            \n",
    "            # ========================================================================\n",
    "            # FEATURE 6: DISTRIBUTION VARIANCE (Alternative Spread Metric)\n",
    "            # ========================================================================\n",
    "            # HOW IT'S COMPUTED:\n",
    "            #   1. Convert counts to probabilities: [p₁, p₂, ..., pₙ]\n",
    "            #   2. Calculate variance: Var = (1/n)Σ(pᵢ - mean_p)²\n",
    "            #   3. Apply scaling factor and clip to [0, 1]\n",
    "            # PURPOSE:\n",
    "            #   - Provides a complementary measure to entropy\n",
    "            #   - Captures how \"spread out\" probabilities are from their mean\n",
    "            #   - HIGH VARIANCE: Very uneven distribution (some outcomes dominant)\n",
    "            #   - LOW VARIANCE: More uniform distribution\n",
    "            #   - Together with entropy, gives agent richer information about distribution shape\n",
    "            # EXAMPLE:\n",
    "            #   - Peaked distribution {'0000': 0.9, others: 0.002 each} -> high variance\n",
    "            #   - Flat distribution {'00': 0.25, '01': 0.25, ...} -> low variance\n",
    "            # ========================================================================\n",
    "            variance = self._compute_distribution_variance(cumulative_outcomes)\n",
    "        else:\n",
    "            # No measurements yet (first step), use neutral default values\n",
    "            entropy = 0.5\n",
    "            variance = 0.5\n",
    "        \n",
    "        # ========================================================================\n",
    "        # FEATURE 7: RATE OF CHANGE (Convergence Velocity Indicator)\n",
    "        # ========================================================================\n",
    "        # HOW IT'S COMPUTED:\n",
    "        #   1. Take the last batch of outcomes from self.outcome_history[-1]\n",
    "        #   2. Take all previous batches and aggregate them\n",
    "        #   3. Normalize both into probability distributions\n",
    "        #   4. Calculate Total Variation Distance (TVD) between them:\n",
    "        #      TVD = 0.5 * Σ|p_prev(x) - p_recent(x)| for all outcomes x\n",
    "        # PURPOSE:\n",
    "        #   - Measures how rapidly the distribution is changing\n",
    "        #   - HIGH RATE (1.0): Distribution still evolving significantly -> Agent should probably continue collecting more shots\n",
    "        #   - LOW RATE (0.0): Distribution has stabilized -> Agent can consider stopping (convergence achieved)\n",
    "        #   - This is a \"forward-looking\" metric that anticipates convergence\n",
    "        # INTUITION:\n",
    "        #   - If adding 50 more shots changes the distribution dramatically -> keep going\n",
    "        #   - If adding 50 more shots barely changes anything -> time to stop\n",
    "        # EXAMPLE:\n",
    "        #   Early in execution: TVD = 0.15 (15% change per batch)\n",
    "        #   Near convergence: TVD = 0.005 (0.5% change per batch)\n",
    "        # ========================================================================\n",
    "        rate_of_change = self._compute_rate_of_change()\n",
    "        \n",
    "        # ========================================================================\n",
    "        # FEATURE 8: RELATIVE PROGRESS (Problem-Size-Aware Progress Metric)\n",
    "        # ========================================================================\n",
    "        # HOW IT'S COMPUTED:\n",
    "        #   1. Estimate typical shots needed based on problem size:\n",
    "        #      typical_shots = size * 500\n",
    "        #      (Heuristic: larger circuits generally need more shots)\n",
    "        #   2. Compute ratio: current_shots / typical_shots\n",
    "        #   3. Clip to maximum of 1.0\n",
    "        # PURPOSE:\n",
    "        #   - Provides context-aware progress information\n",
    "        #   - Helps agent answer: \"Am I close to done, given this problem's size?\"\n",
    "        #   - Different from Feature 4 (shots_norm) which is absolute budget usage\n",
    "        # WHY THIS HELPS:\n",
    "        #   - A 3-qubit circuit at 1000 shots: 1000/(3*500) = 0.67 -> likely near done\n",
    "        #   - A 12-qubit circuit at 1000 shots: 1000/(12*500) = 0.17 -> just getting started\n",
    "        #   - Same absolute shot count, different interpretation based on problem scale\n",
    "        # HEURISTIC JUSTIFICATION:\n",
    "        #   - Larger state space (2^n) requires more sampling for statistical stability\n",
    "        #   - This feature encodes this intuition as a learned signal\n",
    "        # ========================================================================\n",
    "        typical_shots = size * 500\n",
    "        relative_progress = min(self.current_shots / typical_shots, 1.0)\n",
    "\n",
    "        # ============================================================================\n",
    "        # FINAL STATE VECTOR ASSEMBLY\n",
    "        # ============================================================================\n",
    "        # All 8 features are combined into a single numpy array with dtype float32\n",
    "        # This vector is fed directly into the neural network as input\n",
    "        state = np.array([\n",
    "            alg_norm,           # [0] Algorithm type (categorical, normalized)\n",
    "            size_norm,          # [1] Problem size in qubits (continuous, normalized)\n",
    "            backend_norm,       # [2] Hardware backend (categorical, normalized)\n",
    "            shots_norm,         # [3] Absolute shot budget usage (continuous, normalized)\n",
    "            entropy,            # [4] Distribution entropy (statistical, normalized)\n",
    "            variance,           # [5] Distribution variance (statistical, normalized)\n",
    "            rate_of_change,     # [6] Convergence velocity (statistical, normalized)\n",
    "            relative_progress   # [7] Size-aware progress (heuristic, normalized)\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        return state\n",
    "\n",
    "# --- RL Agent and Network ---\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    UPDATED: Now accepts 8 input features instead of 4\n",
    "    Network architecture is also slightly deeper to handle the increased complexity\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, output_size: int):\n",
    "        super().__init__()\n",
    "        # Slightly larger network to accommodate richer state representation\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 512), nn.ReLU(),\n",
    "            nn.Dropout(0.1),  # Light regularization\n",
    "            nn.Linear(512, 256), nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128), nn.ReLU(),\n",
    "            nn.Linear(128, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    The RL agent that encapsulates the DQN model and all the logic for acting, remembering, and learning.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size: int, action_size: int, learning_rate: float = 1e-4, gamma: float = 0.99):\n",
    "        # The main network that the agent uses to make decisions. Its weights are constantly updated.\n",
    "        self.q_net = DQN(state_size, action_size)\n",
    "        # The target network is a copy of the q_net. It is held constant for a period to provide a stable target during training, preventing oscillations\n",
    "        self.target_net = DQN(state_size, action_size)\n",
    "        self.update_target()\n",
    "\n",
    "        # Experience Replay Memory: stores past (state, action, reward, next_state, done) tuples\n",
    "        self.memory = deque(maxlen=200000)\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # --- Hyperparameters ---\n",
    "        # Gamma (discount factor): determines how much the agent values future rewards\n",
    "        # A value closer to 1 makes the agent more \"farsighted\"\n",
    "        self.gamma = gamma\n",
    "        # Epsilon: The exploration rate for the epsilon-greedy policy\n",
    "        # It starts at 1.0 (100% random actions) and decays over time to a minimum value.\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.9998\n",
    "        self.epsilon_min = 0.05\n",
    "        \n",
    "        self.action_size = action_size\n",
    "\n",
    "    def act(self, state: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Chooses an action based on the current state using an epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        # Exploration: with probability epsilon, choose a random action.\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        # Exploitation: with probability 1-epsilon, use the neural network to predict the best action (the one with the highest Q-value)\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Convert to tensor and add batch dimension\n",
    "        with torch.no_grad(): # Disable gradient computation for inference (speeds up computation and saves memory)\n",
    "            q_values = self.q_net(state_tensor)\n",
    "        return torch.argmax(q_values).item()  # Return the action with the highest Q-value\n",
    "\n",
    "    def remember(self, state: np.ndarray, action: int, reward: float, next_state: np.ndarray, done: bool):\n",
    "        \"\"\"Stores an experience tuple in the replay memory\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size: int = 64):\n",
    "        \"\"\"\n",
    "        *** THE CORE LEARNING STEP ***\n",
    "        Samples a batch of past experiences from memory and trains the network using the Bellman equation\n",
    "        \"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return  # Not enough experiences yet to train\n",
    "\n",
    "        # Randomly sample a minibatch of experiences. This breaks correlations in the data.\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # Separate the batch into its components\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        \n",
    "        # Convert to tensors for PyTorch\n",
    "        states = torch.FloatTensor(np.array(states))\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(np.array(next_states))\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        # --- Compute Q-values for the current state using the main network ---\n",
    "        current_q_values = self.q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # --- Compute target Q-values using the Bellman equation and the target network ---\n",
    "        with torch.no_grad():\n",
    "            # Target network is used to provide a stable Q-value estimate for the next state\n",
    "            next_q_values = self.target_net(next_states).max(1)[0]\n",
    "            # Bellman equation: Q_target = reward + gamma * max_a' Q(s', a')\n",
    "            # If the episode is done (terminal state), the future value is 0\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        # --- Calculate loss and update weights ---\n",
    "        # Mean Squared Error between predicted Q and target Q\n",
    "        loss = F.mse_loss(current_q_values, target_q_values)\n",
    "        \n",
    "        # Standard PyTorch training loop: zero gradients, backpropagate, update weights\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Decay epsilon (reduce exploration over time)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def update_target(self):\n",
    "        \"\"\"Copies the weights from the main network to the target network\"\"\"\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "# --- Training Loop ---\n",
    "\n",
    "def train_agent(num_episodes: int = 15000, update_target_every: int = 500) -> Tuple:\n",
    "    \"\"\"\n",
    "    The main training loop for the RL agent\n",
    "    \"\"\"\n",
    "    load_oracle_cache() # Load any previously computed oracle results to save time\n",
    "    \n",
    "    env = IterativeQuantumEnv(\n",
    "        max_shots=20000,\n",
    "        step_size=50,\n",
    "        hard_problem_bias=0.7  # Now 70% of training samples will be hard problems\n",
    "    )\n",
    "    \n",
    "    # Initialize the agent with the correct input size (now 8 instead of 4)\n",
    "    agent = Agent(state_size=8, action_size=2, learning_rate=1e-4, gamma=0.99)\n",
    "    \n",
    "    rewards_history = []\n",
    "    \n",
    "    for episode in tqdm(range(num_episodes), desc=\"Training Episodes\"):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        # Run one episode (one complete problem)\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.replay(batch_size=64)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "        \n",
    "        # Periodically update the target network\n",
    "        if episode % update_target_every == 0 and episode > 0:\n",
    "            agent.update_target()\n",
    "            print(f\"\\nEpisode {episode}: Avg Reward (last 100) = {np.mean(rewards_history[-100:]):.2f}, Epsilon = {agent.epsilon:.3f}\")\n",
    "    \n",
    "    save_oracle_cache()  # Save the oracle cache for future runs\n",
    "    return agent, env, rewards_history\n",
    "\n",
    "def evaluate_agent_sequential(agent: Agent, env: IterativeQuantumEnv, num_tests: int = 200) -> List[Dict]:\n",
    "    \"\"\"Evaluates the final trained agent's performance with exploration turned off\"\"\"\n",
    "    agent.epsilon = 0.0 # Set to a greedy policy (always exploit)\n",
    "    results = []\n",
    "    for _ in tqdm(range(num_tests), desc=\"Evaluation\"):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        info = {}\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            state, _, done, info = env.step(action)\n",
    "        results.append(info)\n",
    "    return results\n",
    "\n",
    "\n",
    "# --- Analysis and Plotting Dashboard ---\n",
    "\n",
    "class AnalysisDashboard:\n",
    "    \"\"\"A class to handle the visualization of the training and evaluation results\"\"\"\n",
    "    def __init__(self, training_rewards: List[float], evaluation_results: List[Dict]):\n",
    "        self.rewards = training_rewards\n",
    "        self.results = evaluation_results\n",
    "        self.shots_used = [r['shots_used'] for r in self.results]\n",
    "        self.optimal_shots = [r['optimal_shots'] for r in self.results]\n",
    "        self.errors = np.array([r['error'] for r in self.results])\n",
    "        self.triplets = [r['triplet'] for r in self.results]\n",
    "\n",
    "    def plot_dashboard(self):\n",
    "        \"\"\"Generates and displays a 2x2 dashboard of performance plots\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "        fig.suptitle(\"Enhanced RL Agent Performance Analysis (8-Feature State + Hard Problem Bias)\", fontsize=20, y=0.98)\n",
    "\n",
    "        self._plot_training_rewards(axes[0, 0])\n",
    "        self._plot_performance_scatter(axes[0, 1])\n",
    "        self._plot_error_distribution(axes[1, 0])\n",
    "        self._plot_error_by_difficulty(axes[1, 1])\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_training_rewards(self, ax: plt.Axes):\n",
    "        \"\"\"\n",
    "        PURPOSE: To visualize the agent's learning progress over time\n",
    "        \"\"\"\n",
    "        ax.plot(self.rewards, label='Total Reward per Episode', alpha=0.2, color='C0')\n",
    "        if len(self.rewards) >= 200:\n",
    "            moving_avg = np.convolve(self.rewards, np.ones(200)/200, mode='valid')\n",
    "            ax.plot(np.arange(199, len(self.rewards)), moving_avg, color='C1', label='200-Episode Moving Average')\n",
    "        ax.axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "        ax.set_title(\"1. Training Rewards Over Time\", fontsize=14)\n",
    "        ax.set_xlabel(\"Episode\")\n",
    "        ax.set_ylabel(\"Total Episode Reward\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--')\n",
    "\n",
    "    def _plot_performance_scatter(self, ax: plt.Axes):\n",
    "        \"\"\"\n",
    "        PURPOSE: To directly compare the agent's final policy against the Oracle's perfect policy\n",
    "        \"\"\"\n",
    "        mae = np.mean(np.abs(self.errors))\n",
    "        ax.scatter(self.optimal_shots, self.shots_used, alpha=0.6, edgecolors='k', s=50)\n",
    "        perfect_range = [0, max(max(self.optimal_shots), max(self.shots_used))]\n",
    "        ax.plot(perfect_range, perfect_range, 'r--', linewidth=2, label='Perfect Policy (y=x)')\n",
    "        ax.set_title(f\"2. Agent Policy vs. Oracle (MAE: {mae:.1f} shots)\", fontsize=14)\n",
    "        ax.set_xlabel(\"Optimal Shots (Determined by Oracle)\")\n",
    "        ax.set_ylabel(\"Shots Used by Agent\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--')\n",
    "\n",
    "    def _plot_error_distribution(self, ax: plt.Axes):\n",
    "        \"\"\"\n",
    "        PURPOSE: To visualize the distribution of the agent's stopping errors\n",
    "        \"\"\"\n",
    "        ax.hist(self.errors, bins=40, edgecolor='k', alpha=0.7)\n",
    "        ax.axvline(0, color='r', linestyle='--', linewidth=2, label='Zero Error (Perfect Stop)')\n",
    "        ax.set_title(\"3. Distribution of Stopping Errors\", fontsize=14)\n",
    "        ax.set_xlabel(\"Error (Agent Shots - Optimal Shots)\")\n",
    "        ax.set_ylabel(\"Frequency of Occurrences\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--')\n",
    "\n",
    "    def _plot_error_by_difficulty(self, ax: plt.Axes):\n",
    "        \"\"\"\n",
    "        Shows how the agent performs on easy vs. hard problems\n",
    "        \"\"\"\n",
    "        easy_errors = [e for e, opt in zip(self.errors, self.optimal_shots) if opt <= 5000]\n",
    "        hard_errors = [e for e, opt in zip(self.errors, self.optimal_shots) if opt > 5000]\n",
    "        \n",
    "        ax.hist([easy_errors, hard_errors], bins=30, label=['Easy (≤5000)', 'Hard (>5000)'], alpha=0.7, edgecolor='k')\n",
    "        ax.axvline(0, color='r', linestyle='--', linewidth=2, label='Perfect')\n",
    "        ax.set_title(\"4. Error Distribution by Problem Difficulty\", fontsize=14)\n",
    "        ax.set_xlabel(\"Error (Agent Shots - Optimal Shots)\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--')\n",
    "\n",
    "    def print_summary_statistics(self):\n",
    "        \"\"\"Generates a detailed text summary of the agent's performance\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ENHANCED AGENT EVALUATION SUMMARY (8 Features + Hard Problem Bias)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Overall metrics\n",
    "        mae = np.mean(np.abs(self.errors))\n",
    "        rmse = np.sqrt(np.mean(self.errors**2))\n",
    "        median_error = np.median(self.errors)\n",
    "        \n",
    "        print(f\"\\nOverall Performance Metrics:\")\n",
    "        print(f\"   • Mean Absolute Error (MAE):    {mae:.1f} shots\")\n",
    "        print(f\"   • Root Mean Squared Error:      {rmse:.1f} shots\")\n",
    "        print(f\"   • Median Error:                 {median_error:.1f} shots\")\n",
    "        \n",
    "        # Breakdown by difficulty\n",
    "        easy_mask = np.array(self.optimal_shots) <= 5000\n",
    "        hard_mask = np.array(self.optimal_shots) > 5000\n",
    "        \n",
    "        if np.any(easy_mask):\n",
    "            easy_mae = np.mean(np.abs(self.errors[easy_mask]))\n",
    "            print(f\"\\nEasy Problems (≤5000 optimal shots):\")\n",
    "            print(f\"   • Count: {np.sum(easy_mask)}\")\n",
    "            print(f\"   • MAE:   {easy_mae:.1f} shots\")\n",
    "        \n",
    "        if np.any(hard_mask):\n",
    "            hard_mae = np.mean(np.abs(self.errors[hard_mask]))\n",
    "            print(f\"\\nHard Problems (>5000 optimal shots):\")\n",
    "            print(f\"   • Count: {np.sum(hard_mask)}\")\n",
    "            print(f\"   • MAE:   {hard_mae:.1f} shots\")\n",
    "        \n",
    "        # Efficiency analysis\n",
    "        overshots = np.sum(self.errors > 0)\n",
    "        undershots = np.sum(self.errors < 0)\n",
    "        perfect = np.sum(self.errors == 0)\n",
    "        \n",
    "        print(f\"\\nDecision Breakdown:\")\n",
    "        print(f\"   • Overshooting (wasteful):      {overshots} ({100*overshots/len(self.errors):.1f}%)\")\n",
    "        print(f\"   • Undershooting (unstable):     {undershots} ({100*undershots/len(self.errors):.1f}%)\")\n",
    "        print(f\"   • Perfect stops:                {perfect} ({100*perfect/len(self.errors):.1f}%)\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ENHANCED QUANTUM SHOT ALLOCATION RL AGENT\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nEnhancements in this version:\")\n",
    "    print(\"  1. Training Set Tuning: 70% of samples are from hard problems (>5000 shots)\")\n",
    "    print(\"  2. Enhanced State Features: 8 features instead of 4\")\n",
    "    print(\"     - Added: entropy, variance, rate of change, relative progress\")\n",
    "    print(\"  3. Deeper Network: More capacity to learn from richer state representation\")\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Train the agent\n",
    "    agent, env, rewards = train_agent(num_episodes=15000, update_target_every=500)\n",
    "    \n",
    "    # Evaluate the agent\n",
    "    print(\"\\nEvaluating trained agent...\")\n",
    "    results = evaluate_agent_sequential(agent, env, num_tests=200)\n",
    "    \n",
    "    # Visualize and analyze results\n",
    "    dashboard = AnalysisDashboard(rewards, results)\n",
    "    dashboard.print_summary_statistics()\n",
    "    dashboard.plot_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
