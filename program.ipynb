{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637e0cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque, Counter\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "from qsimbench import get_outcomes, get_index\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from ast import literal_eval\n",
    "\n",
    "# --- Oracle and Caching System ---\n",
    "# This section defines the \"teacher\" for our RL agent. The Oracle determines\n",
    "# the correct answer (the optimal number of shots), which is used to score\n",
    "# the agent's performance and provide a learning signal (the reward)\n",
    "\n",
    "ORACLE_CACHE = {}\n",
    "CACHE_FILE = \"oracle_cache_4_features.json\"\n",
    "\n",
    "def save_oracle_cache():\n",
    "    \"\"\"\n",
    "    Saves the dictionary of computed optimal shots to a JSON file\n",
    "    This is a critical optimization, as running the Oracle is the most time-consuming part of the program\n",
    "    Saving the results allows us to run the Oracle only once for the entire dataset\n",
    "    \"\"\"\n",
    "    # JSON cannot serialize tuple keys, so we convert them to strings first\n",
    "    string_key_cache = {str(k): v for k, v in ORACLE_CACHE.items()}\n",
    "    with open(CACHE_FILE, 'w') as f:\n",
    "        json.dump(string_key_cache, f)\n",
    "    print(f\"Oracle cache with {len(ORACLE_CACHE)} items saved to {CACHE_FILE}\")\n",
    "\n",
    "def load_oracle_cache():\n",
    "    \"\"\"\n",
    "    Loads the pre-computed optimal shots from the cache file if it exists\n",
    "    This dramatically speeds up subsequent training runs\n",
    "    \"\"\"\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        try:\n",
    "            with open(CACHE_FILE, 'r') as f:\n",
    "                string_key_cache = json.load(f)\n",
    "                # Convert the string keys from the JSON file back into their original tuple format\n",
    "                for k_str, v in string_key_cache.items():\n",
    "                    ORACLE_CACHE[literal_eval(k_str)] = v\n",
    "            print(f\"Loaded {len(ORACLE_CACHE)} items from oracle cache.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not read cache file '{CACHE_FILE}'. Starting empty. Error: {e}\")\n",
    "\n",
    "def find_optimal_shots(algorithm: str, size: int, backend: str, step_size=50, max_shots=20000, threshold=0.01, stability_k=3):\n",
    "    \"\"\"\n",
    "    ================================================================================\n",
    "    *** THE ORACLE ***\n",
    "    ================================================================================\n",
    "    PURPOSE:\n",
    "        This function serves as the \"teacher\" by implementing the Incremental Execution (IE) algorithm described in the paper\n",
    "        It iteratively runs batches of shots and uses a statistical metric to find the point of convergence, which we define as the \"optimal\" shot count.\n",
    "\n",
    "    ROLE IN THE SYSTEM:\n",
    "        Its output (`optimal_shots`) is used ONLY to calculate the final reward for the RL agent during training and as a benchmark during evaluation\n",
    "        The agent itself never gets to see this value while it is making decisions\n",
    "\n",
    "    METHOD:\n",
    "        It determines convergence by measuring the Total Variation Distance (TVD) between the cumulative probability distribution of outcomes at consecutive steps\n",
    "        If the TVD remains below a `threshold` for `stability_k` consecutive iterations, the distribution is considered stable, and the process stops\n",
    "    \"\"\"\n",
    "    # Check if we have already computed this value to save time\n",
    "    cache_key = (algorithm, size, backend)\n",
    "    if cache_key in ORACLE_CACHE:\n",
    "        return ORACLE_CACHE[cache_key]\n",
    "\n",
    "    cumulative_counts = Counter()  # Stores the aggregated outcomes (e.g., {'01': 10, '10': 12})\n",
    "    stable_iterations = 0          # Counter for consecutive stable steps\n",
    "    prev_dist = {}                 # Stores the probability distribution from the previous step\n",
    "\n",
    "    def normalize_dist(counts: Dict[str, int]) -> Dict[str, float]:\n",
    "        \"\"\"Converts raw counts into a normalized probability distribution\"\"\"\n",
    "        total = sum(counts.values())\n",
    "        return {k: v / total for k, v in counts.items()} if total > 0 else {}\n",
    "\n",
    "    def total_variation_distance(p: Dict[str, float], q: Dict[str, float]) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the TVD between two probability distributions, p and q\n",
    "        TVD is a metric of the distance between two distributions\n",
    "        A small TVD (close to 0) means the distributions are very similar\n",
    "        The formula is: TVD(p, q) = 0.5 * sum(|p(i) - q(i)| for all outcomes i)\n",
    "        \"\"\"\n",
    "        all_keys = set(p) | set(q) # Consider all outcomes present in either distribution\n",
    "        return 0.5 * sum(abs(p.get(k, 0) - q.get(k, 0)) for k in all_keys)\n",
    "\n",
    "    # Main loop of the Incremental Execution algorithm\n",
    "    for total_shots_so_far in range(step_size, max_shots + step_size, step_size):\n",
    "        try:\n",
    "            # Use qsimbench to simulate running a new batch of shots\n",
    "            new_batch_counts = get_outcomes(algorithm, size, backend, shots=step_size, strategy='random', exact=True)\n",
    "        except Exception:\n",
    "            # If the simulation fails for any reason, we default to the maximum shot count as a fallback\n",
    "            ORACLE_CACHE[cache_key] = max_shots\n",
    "            return max_shots\n",
    "\n",
    "        cumulative_counts.update(new_batch_counts)\n",
    "        curr_dist = normalize_dist(cumulative_counts)\n",
    "\n",
    "        if prev_dist:\n",
    "            tvd = total_variation_distance(prev_dist, curr_dist)\n",
    "            if tvd < threshold:\n",
    "                stable_iterations += 1\n",
    "            else:\n",
    "                stable_iterations = 0 # Reset if the distribution becomes unstable again\n",
    "\n",
    "            # If the distribution has been stable for `stability_k` steps, we've found the optimal point\n",
    "            if stable_iterations >= stability_k:\n",
    "                ORACLE_CACHE[cache_key] = total_shots_so_far\n",
    "                return total_shots_so_far\n",
    "                \n",
    "        prev_dist = curr_dist\n",
    "        \n",
    "    # If the loop finishes without finding a stable point, the optimal is defined as the maximum allowed shots\n",
    "    ORACLE_CACHE[cache_key] = max_shots\n",
    "    return max_shots\n",
    "\n",
    "# --- The Reinforcement Learning Environment ---\n",
    "\n",
    "class IterativeQuantumEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    This class defines the \"game\" or environment for the RL agent, following the OpenAI Gym interface\n",
    "    An \"episode\" in this game consists of iteratively running batches of shots for a single quantum problem, \n",
    "    where the agent's goal is to decide at each step whether to CONTINUE or STOP\n",
    "    \"\"\"\n",
    "    def __init__(self, max_shots=20000, step_size=50):\n",
    "        super().__init__()\n",
    "        self.max_shots = max_shots\n",
    "        self.step_size = step_size\n",
    "        self.index = get_index()  # Get all available problems from qsimbench\n",
    "        self.all_triplets = self._get_all_noisy_triplets()\n",
    "        self.alg_map, self.backend_map = self._create_mappings()\n",
    "\n",
    "        print(\"Pre-computing optimal shots for all triplets using the oracle...\")\n",
    "        # This loop populates the oracle cache to ensure training is fast.\n",
    "        for triplet in tqdm(self.all_triplets, desc=\"Oracle Pre-computation\"):\n",
    "            find_optimal_shots(triplet[0], triplet[1], triplet[2])\n",
    "        print(\"Oracle pre-computation complete.\")\n",
    "\n",
    "        # Action space: The agent can choose between two discrete actions\n",
    "        # 0: CONTINUE (run another batch of shots)\n",
    "        # 1: STOP (end the episode)\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        # Observation space: A 4-dimensional continuous vector representing the agent's state\n",
    "        # The values are normalized between 0 and 1\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(4,), dtype=np.float32)\n",
    "\n",
    "    def _get_all_noisy_triplets(self) -> List[Tuple[str, int, str]]:\n",
    "        \"\"\"Helper function to parse the qsimbench index and find all valid noisy problems\"\"\"\n",
    "        triplets = []\n",
    "        for alg, sizes in self.index.items():\n",
    "            for size, backends in sizes.items():\n",
    "                if \"aer_simulator\" in backends: # Ensure an ideal, noiseless simulator exists for comparison\n",
    "                    for b in backends:\n",
    "                        if b != \"aer_simulator\": # We only want to train on noisy, realistic backends\n",
    "                            triplets.append((alg, int(size), b))\n",
    "        return triplets\n",
    "\n",
    "    def _create_mappings(self) -> Tuple[Dict[str, int], Dict[str, int]]:\n",
    "        \"\"\"Creates dictionaries to map string names (e.g., 'qaoa') to integer indices for normalization\"\"\"\n",
    "        all_algs = sorted(list(set(t[0] for t in self.all_triplets)))\n",
    "        all_backends = sorted(list(set(t[2] for t in self.all_triplets)))\n",
    "        alg_map = {name: i for i, name in enumerate(all_algs)}\n",
    "        backend_map = {name: i for i, name in enumerate(all_backends)}\n",
    "        return alg_map, backend_map\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Starts a new episode. This is called at the beginning of each training game\n",
    "        It randomly selects a new quantum problem and resets the shot count\n",
    "        \"\"\"\n",
    "        self.current_triplet = random.choice(self.all_triplets)\n",
    "        # Get the pre-computed optimal shots for this problem from the Oracle's cache\n",
    "        self.optimal_shots = find_optimal_shots(*self.current_triplet)\n",
    "        self.current_shots = 0\n",
    "        return self._get_state() # Return the initial state of the new episode\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:\n",
    "        \"\"\"\n",
    "        Processes one step of the game based on the agent's chosen action\n",
    "        \"\"\"\n",
    "        done = False\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        if action == 1:  # Agent chose to STOP\n",
    "            # Terminate the episode and calculate the final, large reward\n",
    "            state, reward, done, info = self._terminate()\n",
    "        else:  # Agent chose to CONTINUE\n",
    "            self.current_shots += self.step_size\n",
    "            if self.current_shots >= self.max_shots:\n",
    "                # If we hit the shot limit, the episode ends automatically\n",
    "                state, reward, done, info = self._terminate()\n",
    "            else:\n",
    "                # If we continue, give a small negative reward (penalty) to encourage\n",
    "                # the agent to finish episodes faster and be more efficient\n",
    "                reward = -0.02\n",
    "                state = self._get_state()\n",
    "\n",
    "        return state, reward, done, info\n",
    "\n",
    "    def _terminate(self) -> Tuple[np.ndarray, float, bool, Dict]:\n",
    "        \"\"\"\n",
    "        Ends the episode and calculates the final reward based on performance\n",
    "        This is where the agent receives its main learning signal\n",
    "        \"\"\"\n",
    "        # Calculate the error: positive for overshooting, negative for undershooting\n",
    "        error = self.current_shots - self.optimal_shots\n",
    "\n",
    "        # --- Asymmetric Reward Shaping Logic ---\n",
    "        # The design of the reward is crucial for guiding the agent's behavior\n",
    "        if error < 0:\n",
    "            # SEVERE PENALTY FOR UNDERSHOOTING: Stopping too early is a critical failure because the result is not statistically stable\n",
    "            # The penalty is proportional to how badly it undershot, teaching the agent that this is the worst possible outcome\n",
    "            final_reward = -1.0 - (abs(error) / self.optimal_shots)\n",
    "        else:\n",
    "            # DECAYING REWARD FOR OVERSHOOTING: This is considered a success, but it becomes less successful as more shots are wasted\n",
    "            # The reward is +1.0 for a perfect stop (error=0) and decays exponentially, encouraging the agent to be precise\n",
    "            final_reward = np.exp(-0.0005 * error)\n",
    "\n",
    "        state = self._get_state()\n",
    "        done = True\n",
    "        info = {\n",
    "            'shots_used': self.current_shots,\n",
    "            'optimal_shots': self.optimal_shots,\n",
    "            'error': error,\n",
    "            'triplet': self.current_triplet,\n",
    "            'final_reward': final_reward\n",
    "        }\n",
    "        return state, final_reward, done, info\n",
    "\n",
    "    def _get_state(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Constructs the agent's state vector. This represents everything the agent is allowed to \"see\" about the world to make its decision\n",
    "        \"\"\"\n",
    "        alg, size, backend = self.current_triplet\n",
    "\n",
    "        # Normalize all features to the [0, 1] range\n",
    "        # This helps the neural network learn more effectively by preventing features with large scales (like shot counts) from dominating features with small scales\n",
    "        alg_norm = self.alg_map.get(alg, 0) / (len(self.alg_map) - 1) if len(self.alg_map) > 1 else 0.5\n",
    "        size_norm = size / 15.0  # Assuming max size is around 15 from the dataset\n",
    "        backend_norm = self.backend_map.get(backend, 0) / (len(self.backend_map) - 1) if len(self.backend_map) > 1 else 0.5\n",
    "        shots_norm = self.current_shots / self.max_shots\n",
    "\n",
    "        state = np.array([alg_norm, size_norm, backend_norm, shots_norm], dtype=np.float32)\n",
    "        return state\n",
    "\n",
    "# --- RL Agent and Network ---\n",
    "# This section defines the \"brain\" of our agent: a Deep Q-Network\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Deep Q-Network (DQN) model. This neural network acts as a function approximator\n",
    "    Instead of a giant table mapping every possible state to an action value (a Q-table), \n",
    "    this network *learns* a function that takes a state and outputs the expected value for each action\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, output_size: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, output_size) # Output layer has one neuron per possible action\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    The RL agent that encapsulates the DQN model and all the logic for acting, remembering, and learning.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size: int, action_size: int, learning_rate: float = 1e-4, gamma: float = 0.99):\n",
    "        # The main network that the agent uses to make decisions. Its weights are constantly updated.\n",
    "        self.q_net = DQN(state_size, action_size)\n",
    "        # The target network is a copy of the q_net. It is held constant for a period to provide a stable target during training, preventing oscillations\n",
    "        self.target_net = DQN(state_size, action_size)\n",
    "        self.update_target()\n",
    "\n",
    "        # Experience Replay Memory: stores past (state, action, reward, next_state, done) tuples\n",
    "        self.memory = deque(maxlen=200000)\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # --- Hyperparameters ---\n",
    "        # Gamma (discount factor): determines how much the agent values future rewards\n",
    "        # A value closer to 1 makes the agent more \"farsighted\"\n",
    "        self.gamma = gamma\n",
    "        # Epsilon: The exploration rate for the epsilon-greedy policy\n",
    "        # It starts at 1.0 (100% random actions) and decays over time to a minimum value.\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.9998\n",
    "        self.epsilon_min = 0.05\n",
    "        \n",
    "        self.action_size = action_size\n",
    "\n",
    "    def act(self, state: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Chooses an action based on the current state using an epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        # Exploration: with probability epsilon, choose a random action.\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        # Exploitation: with probability 1-epsilon, use the neural network to predict the best action (the one with the highest Q-value)\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            return self.q_net(state_tensor).argmax().item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Stores an experience tuple in the replay memory.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train(self, batch_size: int = 128):\n",
    "        \"\"\"\n",
    "        Trains the Q-network by sampling a batch of experiences from memory.\n",
    "        This is the core learning step.\n",
    "        \"\"\"\n",
    "        if len(self.memory) < batch_size * 10: return # Wait for enough memory to be collected\n",
    "\n",
    "        # 1. Sample a random batch of experiences from the replay memory.\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert the batch into PyTorch tensors.\n",
    "        states = torch.from_numpy(np.array(states)).float()\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = torch.from_numpy(np.array(next_states)).float()\n",
    "        dones = torch.BoolTensor(dones).unsqueeze(1)\n",
    "\n",
    "        # 2. Get the agent's predicted Q-values for the actions it actually took\n",
    "        # This is Q(s, a) from the main network\n",
    "        q_values = self.q_net(states).gather(1, actions)\n",
    "\n",
    "        # 3. Calculate the target Q-value. This is based on the Bellman equation:\n",
    "        # Target Q(s, a) = r + gamma * max_a'(Q_target(s', a'))\n",
    "        # We use the target_net to get the value of the next state, which provides a stable target\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            # If an episode was done, there is no future reward, so the second term is zero\n",
    "            target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n",
    "        \n",
    "        # 4. Calculate the loss (Mean Squared Error) between the predicted and target Q-values\n",
    "        loss = F.mse_loss(q_values, target_q_values)\n",
    "\n",
    "        # 5. Update the weights of the main Q-network using backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Decay epsilon to reduce exploration over time\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def update_target(self):\n",
    "        \"\"\"Sync the target network's weights with the main Q-network's weights\"\"\"\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "# --- Main Training and Evaluation ---\n",
    "def train_agent_sequential(episodes: int = 15000) -> Tuple[Agent, IterativeQuantumEnv, List[float]]:\n",
    "    \"\"\"Main training loop that runs the agent through multiple episodes\"\"\"\n",
    "    env = IterativeQuantumEnv()\n",
    "    agent = Agent(env.observation_space.shape[0], env.action_space.n)\n",
    "    rewards_history = []\n",
    "\n",
    "    for ep in tqdm(range(episodes), desc=\"Training Episodes\"):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_episode_reward = 0\n",
    "        \n",
    "        # Run one full episode (one \"game\")\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # The reward stored in memory must be the reward for that specific step\n",
    "            # On the final step, this is the large terminal reward\n",
    "            final_reward = info.get('final_reward', reward)\n",
    "            agent.remember(state, action, final_reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            agent.train() # Perform one training step on a batch from memory\n",
    "        \n",
    "        # For plotting, we calculate the total reward for the episode\n",
    "        total_episode_reward = info.get('final_reward') + (info['shots_used']/env.step_size * -0.02)\n",
    "        rewards_history.append(total_episode_reward)\n",
    "\n",
    "        # Periodically update the target network for stable training\n",
    "        if ep % 25 == 0:\n",
    "            agent.update_target()\n",
    "    \n",
    "    print(\"--- Training Finished ---\")\n",
    "    save_oracle_cache()\n",
    "    return agent, env, rewards_history\n",
    "\n",
    "def evaluate_agent_sequential(agent: Agent, env: IterativeQuantumEnv, num_tests: int = 200) -> List[Dict]:\n",
    "    \"\"\"Evaluates the final trained agent's performance with exploration turned off\"\"\"\n",
    "    agent.epsilon = 0.0 # Set to a greedy policy (always exploit)\n",
    "    results = []\n",
    "    for _ in tqdm(range(num_tests), desc=\"Evaluation\"):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        info = {}\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            state, _, done, info = env.step(action)\n",
    "        results.append(info)\n",
    "    return results\n",
    "\n",
    "\n",
    "# --- Analysis and Plotting Dashboard ---\n",
    "\n",
    "class AnalysisDashboard:\n",
    "    \"\"\"A class to handle the visualization of the training and evaluation results\"\"\"\n",
    "    def __init__(self, training_rewards: List[float], evaluation_results: List[Dict]):\n",
    "        self.rewards = training_rewards\n",
    "        self.results = evaluation_results\n",
    "        self.shots_used = [r['shots_used'] for r in self.results]\n",
    "        self.optimal_shots = [r['optimal_shots'] for r in self.results]\n",
    "        self.errors = np.array([r['error'] for r in self.results])\n",
    "        self.triplets = [r['triplet'] for r in self.results]\n",
    "\n",
    "    def plot_dashboard(self):\n",
    "        \"\"\"Generates and displays a 2x2 dashboard of performance plots\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "        fig.suptitle(\"RL Agent Performance Analysis Dashboard (4-Feature State)\", fontsize=20, y=0.98)\n",
    "\n",
    "        self._plot_training_rewards(axes[0, 0])\n",
    "        self._plot_performance_scatter(axes[0, 1])\n",
    "        self._plot_error_distribution(axes[1, 0])\n",
    "        self._plot_error_by_algorithm(axes[1, 1])\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_training_rewards(self, ax: plt.Axes):\n",
    "        \"\"\"\n",
    "        PURPOSE: To visualize the agent's learning progress over time\n",
    "        \n",
    "        HOW IT WORKS: It plots the raw total reward for each episode, which can be noisy\n",
    "        \n",
    "        More importantly, it overlays a 200-episode moving average (`np.convolve`) to show the underlying trend\n",
    "        A clear upward trend in the moving average indicates that the agent is successfully learning to achieve higher rewards\n",
    "        \"\"\"\n",
    "        ax.plot(self.rewards, label='Total Reward per Episode', alpha=0.2, color='C0')\n",
    "        if len(self.rewards) >= 200:\n",
    "            moving_avg = np.convolve(self.rewards, np.ones(200)/200, mode='valid')\n",
    "            ax.plot(np.arange(199, len(self.rewards)), moving_avg, color='C1', label='200-Episode Moving Average')\n",
    "        ax.axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "        ax.set_title(\"1. Training Rewards Over Time\", fontsize=14)\n",
    "        ax.set_xlabel(\"Episode\")\n",
    "        ax.set_ylabel(\"Total Episode Reward\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--')\n",
    "\n",
    "    def _plot_performance_scatter(self, ax: plt.Axes):\n",
    "        \"\"\"\n",
    "        PURPOSE: To directly compare the agent's final policy against the Oracle's perfect policy\n",
    "        \n",
    "        (This is the most important plot for evaluating the final performance)\n",
    "        HOW IT WORKS: It creates a scatter plot where each point is one evaluation episode\n",
    "        The x-coordinate is the optimal shots from the Oracle, and the y-coordinate is the number of shots the agent chose to use.\n",
    "        \n",
    "        INTERPRETATION:\n",
    "        - The red dashed line (y=x) is the \"Perfect Policy\". Points on this line are perfect decisions\n",
    "        - Points ABOVE the line represent OVERSHOOTING (wasted resources)\n",
    "        - Points BELOW the line represent UNDERSHOOTING (statistically unstable results)\n",
    "        - The Mean Absolute Error (MAE) gives a single number for the average shot deviation\n",
    "        \"\"\"\n",
    "        mae = np.mean(np.abs(self.errors))\n",
    "        ax.scatter(self.optimal_shots, self.shots_used, alpha=0.6, edgecolors='k', s=50)\n",
    "        perfect_range = [0, max(max(self.optimal_shots), max(self.shots_used))]\n",
    "        ax.plot(perfect_range, perfect_range, 'r--', linewidth=2, label='Perfect Policy (y=x)')\n",
    "        ax.set_title(f\"2. Agent Policy vs. Oracle (MAE: {mae:.1f} shots)\", fontsize=14)\n",
    "        ax.set_xlabel(\"Optimal Shots (Determined by Oracle)\")\n",
    "        ax.set_ylabel(\"Shots Used by Agent\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--')\n",
    "\n",
    "    def _plot_error_distribution(self, ax: plt.Axes):\n",
    "        \"\"\"\n",
    "        PURPOSE: To visualize the distribution of the agent's stopping errors\n",
    "\n",
    "        HOW IT WORKS: It plots a histogram of the errors (`agent_shots - optimal_shots`)\n",
    "\n",
    "        INTERPRETATION: This shows the agent's tendencies. A distribution skewed to the right of zero (positive error) indicates a cautious agent that tends to overshoot\n",
    "        A distribution skewed to the left indicates a risky agent that tends to undershoot\n",
    "        A tight distribution centered at zero is the ideal result\n",
    "        \"\"\"\n",
    "        ax.hist(self.errors, bins=40, edgecolor='k', alpha=0.7)\n",
    "        ax.axvline(0, color='r', linestyle='--', linewidth=2, label='Zero Error (Perfect Stop)')\n",
    "        ax.set_title(\"3. Distribution of Stopping Errors\", fontsize=14)\n",
    "        ax.set_xlabel(\"Error (Agent Shots - Optimal Shots)\")\n",
    "        ax.set_ylabel(\"Frequency of Occurrences\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--')\n",
    "\n",
    "    def _plot_error_by_algorithm(self, ax: plt.Axes):\n",
    "        \"\"\"\n",
    "        PURPOSE: To see if the agent's performance varies across different types of quantum algorithms\n",
    "        \n",
    "        HOW IT WORKS: It groups the evaluation results by algorithm type and calculates the average error for each\n",
    "        It then displays this as a bar chart\n",
    "        \n",
    "        INTERPRETATION: This helps identify if the agent has learned a generalizable policy\n",
    "        or if it struggles with specific, perhaps more complex, types of quantum circuits.\n",
    "        Large bars for certain algorithms indicate areas for improvement.\n",
    "        \"\"\"\n",
    "        algs = [t[0] for t in self.triplets]\n",
    "        unique_algs = sorted(list(set(algs)))\n",
    "        mean_errors_by_alg = []\n",
    "        \n",
    "        for alg in unique_algs:\n",
    "            # Get all errors for the current algorithm\n",
    "            errors_for_alg = [self.errors[i] for i, a in enumerate(algs) if a == alg]\n",
    "            if errors_for_alg:\n",
    "                mean_errors_by_alg.append(np.mean(errors_for_alg))\n",
    "            else:\n",
    "                mean_errors_by_alg.append(0)\n",
    "\n",
    "        ax.bar(unique_algs, mean_errors_by_alg, edgecolor='k', alpha=0.7, color='C2')\n",
    "        ax.axhline(0, color='r', linestyle='--', linewidth=2, label='Zero Error')\n",
    "        ax.set_title(\"4. Average Error by Algorithm Type\", fontsize=14)\n",
    "        ax.set_xlabel(\"Quantum Algorithm\")\n",
    "        ax.set_ylabel(\"Average Error (Agent Shots - Optimal)\")\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=8) # Rotate labels for readability\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the oracle cache to speed up the run if it exists\n",
    "    load_oracle_cache()\n",
    "\n",
    "    # Train the agent\n",
    "    trained_agent, trained_env, rewards_history = train_agent_sequential(episodes=10000)\n",
    "\n",
    "    # Evaluate the final trained policy on 500 random test cases\n",
    "    evaluation_results = evaluate_agent_sequential(trained_agent, trained_env, num_tests=500)\n",
    "\n",
    "    # Generate and show the performance dashboard\n",
    "    dashboard = AnalysisDashboard(rewards_history, evaluation_results)\n",
    "    dashboard.plot_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
